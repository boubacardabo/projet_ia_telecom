- opt models and dolly are too small and they do not generate anything (llm_output=null) when invoked
- you can set langchain.debug=True to enable debug in terminal
- when you get this message "", this means your propt has too much token and the model might not be able to handle it ===> increase max_new_tokens or reduce prompt content